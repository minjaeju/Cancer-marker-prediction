{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6016, 6016)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('scoremat.pkl','rb') as f:\n",
    "    mat = pickle.load(f)\n",
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = mat*1000\n",
    "mat1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.diagonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[0.00016622 0.00016622 0.00016622 ... 0.00016622 0.00016622 0.00016622]\n",
      "0.0\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(mat.max())\n",
    "print(mat.mean(1))\n",
    "print(mat.min())\n",
    "print(mat.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f7ccc9f02d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(mat[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = torch.tensor([[ 3.1348e+00, -1.4505e-01, -1.4832e+00, -2.8843e+00, -1.2542e+00,  8.3138e-01, -1.0000e+09, -1.0000e+09],\n",
    "                   [ -1.4505e-01, 3.1348e+00,  -1.4832e+00, -1.2542e+00, -2.8843e+00,   8.3138e-01, -1.0000e+09, -1.0000e+09]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8604, 0.0324, 0.0085, 0.0021, 0.0107, 0.0860, 0.0000, 0.0000],\n",
       "        [0.0324, 0.8604, 0.0085, 0.0107, 0.0021, 0.0860, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = nn.Softmax(dim=-1)(num)\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5, 1, 4, 2],\n",
       "        [1, 5, 0, 3, 2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,index = torch.topk(sm,5,dim=-1)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2,8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "for j in range(2):\n",
    "    y = x[j]\n",
    "    #print(y)\n",
    "    for k in range(5):\n",
    "        y[index[j][k]]=1\n",
    "    z.append(y)\n",
    "    q = torch.stack(z,dim=0)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"./data/preprocessed/tcga_brca_dataset.pkl\",'rb')\n",
    "# data_list = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "#f = open(\"./data/preprocessed/cv/tcga_brca_dataset_train_0.pkl\",'rb')\n",
    "#train_list = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "#f = open(\"./data/preprocessed/cv/tcga_brca_dataset_test_0.pkl\",'rb')\n",
    "#test_list = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_sample_train_0.pkl\",'rb')\n",
    "train_list = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_sample_test_0.pkl\",'rb')\n",
    "test_list = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"./data/preprocessed/tcga_brca_dataset.pkl\",'rb')\n",
    "# data_list = pickle.load(f)\n",
    "# f.close()\n",
    "#netics\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_sample_train_0.pkl\",'rb')\n",
    "trainlist_sample = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_sample_test_0.pkl\",'rb')\n",
    "testlist_sample = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_TF_train_0.pkl\",'rb')\n",
    "trainlist_TF = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"./data/preprocessed/cv/netics_exp_TF_test_0.pkl\",'rb')\n",
    "testlist_TF = pickle.load(f)\n",
    "f.close()\n",
    "#len(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_list,batch_size=1)\n",
    "test_loader = DataLoader(test_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netics\n",
    "trainloader_sample = DataLoader(trainlist_sample,batch_size=1)\n",
    "testloader_sample = DataLoader(testlist_sample, batch_size=1)\n",
    "trainloader_TF = DataLoader(trainlist_TF, batch_size=1)\n",
    "testloader_TF = DataLoader(testlist_TF, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleGCN_SAGPOOL:4 layers\n",
      "cuda\n",
      "Save path: ./data/model/simpleGCN_SAGPOOL\n"
     ]
    }
   ],
   "source": [
    "#num_genes =9630\n",
    "#netics\n",
    "num_genes = 6016\n",
    "num_class = 5\n",
    "num_layers = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.simpleGCN_SAGPOOL(num_genes,num_class,num_layers=num_layers).to(device)\n",
    "#model = model.pagerank_SAGPOOL(num_genes,num_class).to(device)\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "save_path = \"./data/model/simpleGCN_SAGPOOL\"\n",
    "print(\"Save path:\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_op(output, data.y)\n",
    "        total_loss+=loss.item()*data.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        out = model(data.to(device))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        preds.append(pred.cpu())\n",
    "    \n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='weighted') if pred.sum() > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netics\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data1, data2 in zip(trainloader_sample, trainloader_TF):\n",
    "        data1 = data1.to(device)\n",
    "        data2 = data2.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data1, data2)\n",
    "        loss = loss_op(output, data1.y)\n",
    "        total_loss+=loss.item()*data1.num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                    \n",
    "    return total_loss / len(trainloader_sample.dataset)\n",
    "\n",
    "def test(loader1, loader2):\n",
    "    model.eval()\n",
    "\n",
    "    ys, preds = [], []\n",
    "    for data1, data2 in zip(loader1, loader2):\n",
    "        ys.append(data1.y)\n",
    "        out = model(data1.to(device),data2.to(device))\n",
    "        pred = out.max(dim=1)[1]\n",
    "        preds.append(pred.cpu())\n",
    "    \n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='weighted') #if pred.sum() > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minjae/miniconda3/envs/minjae/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.3938, Test: 0.1608\n",
      "Epoch: 02, Loss: 1.3769, Test: 0.1608\n",
      "Epoch: 03, Loss: 1.3743, Test: 0.1608\n",
      "Epoch: 04, Loss: 1.3726, Test: 0.1608\n",
      "Epoch: 05, Loss: 1.3716, Test: 0.1608\n",
      "Epoch: 06, Loss: 1.3703, Test: 0.1608\n",
      "Epoch: 07, Loss: 1.3684, Test: 0.1608\n",
      "Epoch: 08, Loss: 1.3649, Test: 0.1608\n",
      "Epoch: 09, Loss: 1.3617, Test: 0.1608\n",
      "Epoch: 10, Loss: 1.3550, Test: 0.1608\n",
      "Epoch: 11, Loss: 1.3471, Test: 0.1608\n",
      "Epoch: 12, Loss: 1.3393, Test: 0.1608\n",
      "Epoch: 13, Loss: 1.3296, Test: 0.1608\n",
      "Epoch: 14, Loss: 1.3188, Test: 0.1608\n",
      "Epoch: 15, Loss: 1.3037, Test: 0.1608\n",
      "Epoch: 16, Loss: 1.2913, Test: 0.1608\n",
      "Epoch: 17, Loss: 1.2749, Test: 0.1608\n",
      "Epoch: 18, Loss: 1.2579, Test: 0.1569\n",
      "Epoch: 19, Loss: 1.2411, Test: 0.1620\n",
      "Epoch: 20, Loss: 1.2217, Test: 0.1582\n",
      "Epoch: 21, Loss: 1.2040, Test: 0.1594\n",
      "Epoch: 22, Loss: 1.1912, Test: 0.1594\n",
      "Epoch: 23, Loss: 1.1743, Test: 0.1607\n",
      "Epoch: 24, Loss: 1.1561, Test: 0.1633\n",
      "Epoch: 25, Loss: 1.1428, Test: 0.1593\n",
      "Epoch: 26, Loss: 1.1275, Test: 0.1540\n",
      "Epoch: 27, Loss: 1.1134, Test: 0.1725\n",
      "Epoch: 28, Loss: 1.0940, Test: 0.2085\n",
      "Epoch: 29, Loss: 1.0767, Test: 0.2088\n",
      "Epoch: 30, Loss: 1.0574, Test: 0.2195\n",
      "Epoch: 31, Loss: 1.0385, Test: 0.2054\n",
      "Epoch: 32, Loss: 1.0198, Test: 0.2212\n",
      "Epoch: 33, Loss: 0.9998, Test: 0.2350\n",
      "Epoch: 34, Loss: 0.9816, Test: 0.2246\n",
      "Epoch: 35, Loss: 0.9629, Test: 0.2193\n",
      "Epoch: 36, Loss: 0.9456, Test: 0.2196\n",
      "Epoch: 37, Loss: 0.9265, Test: 0.2258\n",
      "Epoch: 38, Loss: 0.9050, Test: 0.2258\n",
      "Epoch: 39, Loss: 0.8863, Test: 0.2329\n",
      "Epoch: 40, Loss: 0.8672, Test: 0.2223\n",
      "Epoch: 41, Loss: 0.8480, Test: 0.2350\n",
      "Epoch: 42, Loss: 0.8304, Test: 0.2355\n",
      "Epoch: 43, Loss: 0.8121, Test: 0.2361\n",
      "Epoch: 44, Loss: 0.7962, Test: 0.2367\n",
      "Epoch: 45, Loss: 0.7781, Test: 0.2670\n",
      "Epoch: 46, Loss: 0.7623, Test: 0.2670\n",
      "Epoch: 47, Loss: 0.7449, Test: 0.2585\n",
      "Epoch: 48, Loss: 0.7282, Test: 0.2760\n",
      "Epoch: 49, Loss: 0.7135, Test: 0.2889\n",
      "Epoch: 50, Loss: 0.6985, Test: 0.2869\n",
      "Epoch: 51, Loss: 0.6832, Test: 0.2750\n",
      "Epoch: 52, Loss: 0.6666, Test: 0.2593\n",
      "Epoch: 53, Loss: 0.6515, Test: 0.2593\n",
      "Epoch: 54, Loss: 0.6354, Test: 0.2593\n",
      "Epoch: 55, Loss: 0.6225, Test: 0.2825\n",
      "Epoch: 56, Loss: 0.6093, Test: 0.2825\n",
      "Epoch: 57, Loss: 0.5958, Test: 0.2825\n",
      "Epoch: 58, Loss: 0.5827, Test: 0.2823\n",
      "Epoch: 59, Loss: 0.5706, Test: 0.2823\n",
      "Epoch: 60, Loss: 0.5645, Test: 0.2823\n",
      "Epoch: 61, Loss: 0.5473, Test: 0.2823\n",
      "Epoch: 62, Loss: 0.5359, Test: 0.2823\n",
      "Epoch: 63, Loss: 0.5254, Test: 0.2662\n",
      "Epoch: 64, Loss: 0.5144, Test: 0.2662\n",
      "Epoch: 65, Loss: 0.5042, Test: 0.2811\n",
      "Epoch: 66, Loss: 0.4941, Test: 0.2940\n",
      "Epoch: 67, Loss: 0.4844, Test: 0.2940\n",
      "Epoch: 68, Loss: 0.4748, Test: 0.3019\n",
      "Epoch: 69, Loss: 0.4654, Test: 0.2940\n",
      "Epoch: 70, Loss: 0.4560, Test: 0.2940\n",
      "Epoch: 71, Loss: 0.4479, Test: 0.2940\n",
      "Epoch: 72, Loss: 0.4388, Test: 0.3019\n",
      "Epoch: 73, Loss: 0.4307, Test: 0.2878\n",
      "Epoch: 74, Loss: 0.4238, Test: 0.3156\n",
      "Epoch: 75, Loss: 0.4155, Test: 0.2959\n",
      "Epoch: 76, Loss: 0.4071, Test: 0.2959\n",
      "Epoch: 77, Loss: 0.3991, Test: 0.3065\n",
      "Epoch: 78, Loss: 0.3915, Test: 0.3037\n",
      "Epoch: 79, Loss: 0.3837, Test: 0.3144\n",
      "Epoch: 80, Loss: 0.3776, Test: 0.3049\n",
      "Epoch: 81, Loss: 0.3704, Test: 0.3049\n",
      "Epoch: 82, Loss: 0.3636, Test: 0.3129\n",
      "Epoch: 83, Loss: 0.3563, Test: 0.3239\n",
      "Epoch: 84, Loss: 0.3499, Test: 0.3523\n",
      "Epoch: 85, Loss: 0.3458, Test: 0.3209\n",
      "Epoch: 86, Loss: 0.3399, Test: 0.3299\n",
      "Epoch: 87, Loss: 0.3332, Test: 0.3299\n",
      "Epoch: 88, Loss: 0.3253, Test: 0.3339\n",
      "Epoch: 89, Loss: 0.3194, Test: 0.3473\n",
      "Epoch: 90, Loss: 0.3220, Test: 0.3390\n",
      "Epoch: 91, Loss: 0.3168, Test: 0.3512\n",
      "Epoch: 92, Loss: 0.3271, Test: 0.3427\n",
      "Epoch: 93, Loss: 0.3069, Test: 0.3582\n",
      "Epoch: 94, Loss: 0.3085, Test: 0.3413\n",
      "Epoch: 95, Loss: 0.3034, Test: 0.3484\n",
      "Epoch: 96, Loss: 0.2959, Test: 0.3484\n",
      "Epoch: 97, Loss: 0.2901, Test: 0.3577\n",
      "Epoch: 98, Loss: 0.2803, Test: 0.3577\n",
      "Epoch: 99, Loss: 0.2821, Test: 0.3404\n",
      "Epoch: 100, Loss: 0.2793, Test: 0.3497\n",
      "Epoch: 101, Loss: 0.3016, Test: 0.3110\n",
      "Epoch: 102, Loss: 0.2798, Test: 0.2929\n",
      "Epoch: 103, Loss: 0.2697, Test: 0.2677\n",
      "Epoch: 104, Loss: 0.2673, Test: 0.2593\n",
      "Epoch: 105, Loss: 0.2606, Test: 0.2595\n",
      "Epoch: 106, Loss: 0.2659, Test: 0.2594\n",
      "Epoch: 107, Loss: 0.3180, Test: 0.2502\n",
      "Epoch: 108, Loss: 0.2794, Test: 0.3075\n",
      "Epoch: 109, Loss: 0.2566, Test: 0.3213\n",
      "Epoch: 110, Loss: 0.2711, Test: 0.3121\n",
      "Epoch: 111, Loss: 0.2375, Test: 0.2752\n",
      "Epoch: 112, Loss: 0.2308, Test: 0.2815\n",
      "Epoch: 113, Loss: 0.2198, Test: 0.2775\n",
      "Epoch: 114, Loss: 0.2332, Test: 0.3311\n",
      "Epoch: 115, Loss: 0.2104, Test: 0.3221\n",
      "Epoch: 116, Loss: 0.2060, Test: 0.3221\n",
      "Epoch: 117, Loss: 0.2016, Test: 0.3344\n",
      "Epoch: 118, Loss: 0.1976, Test: 0.3344\n",
      "Epoch: 119, Loss: 0.1945, Test: 0.3344\n",
      "Epoch: 120, Loss: 0.1922, Test: 0.3254\n",
      "Epoch: 121, Loss: 0.1890, Test: 0.3344\n",
      "Epoch: 122, Loss: 0.1853, Test: 0.3226\n",
      "Epoch: 123, Loss: 0.1808, Test: 0.3134\n",
      "Epoch: 124, Loss: 0.1781, Test: 0.3136\n",
      "Epoch: 125, Loss: 0.1743, Test: 0.3254\n",
      "Epoch: 126, Loss: 0.1708, Test: 0.3160\n",
      "Epoch: 127, Loss: 0.1679, Test: 0.3178\n",
      "Epoch: 128, Loss: 0.1634, Test: 0.3178\n",
      "Epoch: 129, Loss: 0.1598, Test: 0.3160\n",
      "Epoch: 130, Loss: 0.1559, Test: 0.3160\n",
      "Epoch: 131, Loss: 0.1503, Test: 0.3250\n",
      "Epoch: 132, Loss: 0.1473, Test: 0.3237\n",
      "Epoch: 133, Loss: 0.1444, Test: 0.3151\n",
      "Epoch: 134, Loss: 0.1421, Test: 0.3235\n",
      "Epoch: 135, Loss: 0.1394, Test: 0.3145\n",
      "Epoch: 136, Loss: 0.1373, Test: 0.3145\n",
      "Epoch: 137, Loss: 0.1350, Test: 0.3125\n",
      "Epoch: 138, Loss: 0.1317, Test: 0.3123\n",
      "Epoch: 139, Loss: 0.1286, Test: 0.3123\n",
      "Epoch: 140, Loss: 0.1263, Test: 0.3140\n",
      "Epoch: 141, Loss: 0.1233, Test: 0.3123\n",
      "Epoch: 142, Loss: 0.1209, Test: 0.3028\n",
      "Epoch: 143, Loss: 0.1194, Test: 0.3045\n",
      "Epoch: 144, Loss: 0.1166, Test: 0.3045\n",
      "Epoch: 145, Loss: 0.1149, Test: 0.3045\n",
      "Epoch: 146, Loss: 0.1128, Test: 0.3045\n",
      "Epoch: 147, Loss: 0.1101, Test: 0.3045\n",
      "Epoch: 148, Loss: 0.1087, Test: 0.3045\n",
      "Epoch: 149, Loss: 0.1069, Test: 0.3063\n",
      "Epoch: 150, Loss: 0.1056, Test: 0.3404\n",
      "Epoch: 151, Loss: 0.1059, Test: 0.3404\n",
      "Epoch: 152, Loss: 0.1042, Test: 0.3404\n",
      "Epoch: 153, Loss: 0.1027, Test: 0.3403\n",
      "Epoch: 154, Loss: 0.1010, Test: 0.3403\n",
      "Epoch: 155, Loss: 0.0978, Test: 0.3403\n",
      "Epoch: 156, Loss: 0.0961, Test: 0.3403\n",
      "Epoch: 157, Loss: 0.0950, Test: 0.3417\n",
      "Epoch: 158, Loss: 0.0943, Test: 0.3317\n",
      "Epoch: 159, Loss: 0.0915, Test: 0.3302\n",
      "Epoch: 160, Loss: 0.0896, Test: 0.3418\n",
      "Epoch: 161, Loss: 0.0886, Test: 0.3417\n",
      "Epoch: 162, Loss: 0.0873, Test: 0.3418\n",
      "Epoch: 163, Loss: 0.0867, Test: 0.3417\n",
      "Epoch: 164, Loss: 0.0852, Test: 0.3417\n",
      "Epoch: 165, Loss: 0.0839, Test: 0.3417\n",
      "Epoch: 166, Loss: 0.0821, Test: 0.3417\n",
      "Epoch: 167, Loss: 0.0800, Test: 0.3320\n",
      "Epoch: 168, Loss: 0.0802, Test: 0.3235\n",
      "Epoch: 169, Loss: 0.0797, Test: 0.3235\n",
      "Epoch: 170, Loss: 0.0784, Test: 0.3235\n",
      "Epoch: 171, Loss: 0.0776, Test: 0.3235\n",
      "Epoch: 172, Loss: 0.0762, Test: 0.3235\n",
      "Epoch: 173, Loss: 0.0749, Test: 0.3394\n",
      "Epoch: 174, Loss: 0.0750, Test: 0.3394\n",
      "Epoch: 175, Loss: 0.0740, Test: 0.3394\n",
      "Epoch: 176, Loss: 0.0731, Test: 0.3305\n",
      "Epoch: 177, Loss: 0.0722, Test: 0.3442\n",
      "Epoch: 178, Loss: 0.0713, Test: 0.3442\n",
      "Epoch: 179, Loss: 0.0707, Test: 0.3440\n",
      "Epoch: 180, Loss: 0.0701, Test: 0.3440\n",
      "Epoch: 181, Loss: 0.0689, Test: 0.3440\n",
      "Epoch: 182, Loss: 0.0685, Test: 0.3539\n",
      "Epoch: 183, Loss: 0.0667, Test: 0.3539\n",
      "Epoch: 184, Loss: 0.0663, Test: 0.3440\n",
      "Epoch: 185, Loss: 0.0653, Test: 0.3543\n",
      "Epoch: 186, Loss: 0.0644, Test: 0.3543\n",
      "Epoch: 187, Loss: 0.0633, Test: 0.3543\n",
      "Epoch: 188, Loss: 0.0629, Test: 0.3543\n",
      "Epoch: 189, Loss: 0.0628, Test: 0.3543\n",
      "Epoch: 190, Loss: 0.0615, Test: 0.3543\n",
      "Epoch: 191, Loss: 0.0608, Test: 0.3639\n",
      "Epoch: 192, Loss: 0.0602, Test: 0.3639\n",
      "Epoch: 193, Loss: 0.0596, Test: 0.3736\n",
      "Epoch: 194, Loss: 0.0590, Test: 0.3736\n",
      "Epoch: 195, Loss: 0.0586, Test: 0.3736\n",
      "Epoch: 196, Loss: 0.0580, Test: 0.3727\n",
      "Epoch: 197, Loss: 0.0573, Test: 0.3738\n",
      "Epoch: 198, Loss: 0.0568, Test: 0.3738\n",
      "Epoch: 199, Loss: 0.0560, Test: 0.3738\n",
      "Epoch: 200, Loss: 0.0555, Test: 0.3631\n",
      "Epoch: 201, Loss: 0.0551, Test: 0.3738\n",
      "Epoch: 202, Loss: 0.0548, Test: 0.3738\n",
      "Epoch: 203, Loss: 0.0540, Test: 0.3738\n",
      "Epoch: 204, Loss: 0.0531, Test: 0.3738\n",
      "Epoch: 205, Loss: 0.0531, Test: 0.3738\n",
      "Epoch: 206, Loss: 0.0528, Test: 0.3738\n",
      "Epoch: 207, Loss: 0.0526, Test: 0.3751\n",
      "Epoch: 208, Loss: 0.0525, Test: 0.3624\n",
      "Epoch: 209, Loss: 0.0527, Test: 0.3751\n",
      "Epoch: 210, Loss: 0.0527, Test: 0.3751\n",
      "Epoch: 211, Loss: 0.0520, Test: 0.3634\n",
      "Epoch: 212, Loss: 0.0509, Test: 0.3751\n",
      "Epoch: 213, Loss: 0.0510, Test: 0.3751\n",
      "Epoch: 214, Loss: 0.0505, Test: 0.3634\n",
      "Epoch: 215, Loss: 0.0492, Test: 0.3634\n",
      "Epoch: 216, Loss: 0.0499, Test: 0.3626\n",
      "Epoch: 217, Loss: 0.0495, Test: 0.3626\n",
      "Epoch: 218, Loss: 0.0490, Test: 0.3647\n",
      "Epoch: 219, Loss: 0.0495, Test: 0.3626\n",
      "Epoch: 220, Loss: 0.0488, Test: 0.3640\n",
      "Epoch: 221, Loss: 0.0484, Test: 0.3746\n",
      "Epoch: 222, Loss: 0.0486, Test: 0.3749\n",
      "Epoch: 223, Loss: 0.0485, Test: 0.3751\n",
      "Epoch: 224, Loss: 0.0486, Test: 0.3738\n",
      "Epoch: 225, Loss: 0.0487, Test: 0.3614\n",
      "Epoch: 226, Loss: 0.0482, Test: 0.3614\n",
      "Epoch: 227, Loss: 0.0479, Test: 0.3733\n",
      "Epoch: 228, Loss: 0.0473, Test: 0.3614\n",
      "Epoch: 229, Loss: 0.0477, Test: 0.3634\n",
      "Epoch: 230, Loss: 0.0470, Test: 0.3634\n",
      "Epoch: 231, Loss: 0.0471, Test: 0.3634\n",
      "Epoch: 232, Loss: 0.0470, Test: 0.3524\n",
      "Epoch: 233, Loss: 0.0475, Test: 0.3524\n",
      "Epoch: 234, Loss: 0.0468, Test: 0.3426\n",
      "Epoch: 235, Loss: 0.0468, Test: 0.3535\n",
      "Epoch: 236, Loss: 0.0465, Test: 0.3329\n",
      "Epoch: 237, Loss: 0.0476, Test: 0.3453\n",
      "Epoch: 238, Loss: 0.0474, Test: 0.3559\n",
      "Epoch: 239, Loss: 0.0471, Test: 0.3547\n",
      "Epoch: 240, Loss: 0.0470, Test: 0.3564\n",
      "Epoch: 241, Loss: 0.0462, Test: 0.3340\n",
      "Epoch: 242, Loss: 0.0457, Test: 0.3210\n",
      "Epoch: 243, Loss: 0.0451, Test: 0.3197\n",
      "Epoch: 244, Loss: 0.0448, Test: 0.3330\n",
      "Epoch: 245, Loss: 0.0439, Test: 0.3252\n",
      "Epoch: 246, Loss: 0.0448, Test: 0.3262\n",
      "Epoch: 247, Loss: 0.0428, Test: 0.3262\n",
      "Epoch: 248, Loss: 0.0432, Test: 0.3262\n",
      "Epoch: 249, Loss: 0.0431, Test: 0.3375\n",
      "Epoch: 250, Loss: 0.0437, Test: 0.3370\n",
      "Epoch: 251, Loss: 0.0428, Test: 0.3567\n",
      "Epoch: 252, Loss: 0.0429, Test: 0.3448\n",
      "Epoch: 253, Loss: 0.0425, Test: 0.3448\n",
      "Epoch: 254, Loss: 0.0418, Test: 0.3448\n",
      "Epoch: 255, Loss: 0.0411, Test: 0.3567\n",
      "Epoch: 256, Loss: 0.0416, Test: 0.3567\n",
      "Epoch: 257, Loss: 0.0411, Test: 0.3563\n",
      "Epoch: 258, Loss: 0.0414, Test: 0.3563\n",
      "Epoch: 259, Loss: 0.0415, Test: 0.3444\n",
      "Epoch: 260, Loss: 0.0407, Test: 0.3444\n",
      "Epoch: 261, Loss: 0.0407, Test: 0.3448\n",
      "Epoch: 262, Loss: 0.0412, Test: 0.3567\n",
      "Epoch: 263, Loss: 0.0404, Test: 0.3527\n",
      "Epoch: 264, Loss: 0.0404, Test: 0.3527\n",
      "Epoch: 265, Loss: 0.0404, Test: 0.3527\n",
      "Epoch: 266, Loss: 0.0401, Test: 0.3527\n",
      "Epoch: 267, Loss: 0.0394, Test: 0.3466\n",
      "Epoch: 268, Loss: 0.0389, Test: 0.3355\n",
      "Epoch: 269, Loss: 0.0398, Test: 0.3448\n",
      "Epoch: 270, Loss: 0.0397, Test: 0.3567\n",
      "Epoch: 271, Loss: 0.0394, Test: 0.3448\n",
      "Epoch: 272, Loss: 0.0391, Test: 0.3434\n",
      "Epoch: 273, Loss: 0.0404, Test: 0.3553\n",
      "Epoch: 274, Loss: 0.0404, Test: 0.3434\n",
      "Epoch: 275, Loss: 0.0403, Test: 0.3434\n",
      "Epoch: 276, Loss: 0.0423, Test: 0.3434\n",
      "Epoch: 277, Loss: 0.0399, Test: 0.3436\n",
      "Epoch: 278, Loss: 0.0404, Test: 0.3348\n",
      "Epoch: 279, Loss: 0.0400, Test: 0.3471\n",
      "Epoch: 280, Loss: 0.0396, Test: 0.3434\n",
      "Epoch: 281, Loss: 0.0396, Test: 0.3558\n",
      "Epoch: 282, Loss: 0.0393, Test: 0.3303\n",
      "Epoch: 283, Loss: 0.0390, Test: 0.3525\n",
      "Epoch: 284, Loss: 0.0385, Test: 0.3445\n",
      "Epoch: 285, Loss: 0.0390, Test: 0.3356\n",
      "Epoch: 286, Loss: 0.0387, Test: 0.3442\n",
      "Epoch: 287, Loss: 0.0390, Test: 0.3356\n",
      "Epoch: 288, Loss: 0.0390, Test: 0.3356\n",
      "Epoch: 289, Loss: 0.0392, Test: 0.3254\n",
      "Epoch: 290, Loss: 0.0392, Test: 0.3474\n",
      "Epoch: 291, Loss: 0.0390, Test: 0.3370\n",
      "Epoch: 292, Loss: 0.0390, Test: 0.3442\n",
      "Epoch: 293, Loss: 0.0389, Test: 0.3572\n",
      "Epoch: 294, Loss: 0.0395, Test: 0.3560\n",
      "Epoch: 295, Loss: 0.0374, Test: 0.3558\n",
      "Epoch: 296, Loss: 0.0379, Test: 0.3225\n",
      "Epoch: 297, Loss: 0.0376, Test: 0.3433\n",
      "Epoch: 298, Loss: 0.0375, Test: 0.3316\n",
      "Epoch: 299, Loss: 0.0413, Test: 0.3433\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/minjae/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minjae/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4c9910726ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_epoch{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minjae/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/minjae/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "        \n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        torch.save(model.state_dict(),save_path+\"/model_epoch{}\".format(epoch))\n",
    "\n",
    "    test_f1 = test(test_loader)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Test: {:.4f}'.format(epoch, loss, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netics\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "        \n",
    "for epoch in range(1, 1001):\n",
    "    loss = train()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        torch.save(model.state_dict(),save_path+\"/model_epoch{}\".format(epoch))\n",
    "\n",
    "    test_f1 = test(testloader_sample, testloader_TF)\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Test: {:.4f}'.format(epoch, loss, test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "@torch.no_grad()\n",
    "#https://github.com/rusty1s/pytorch_geometric/blob/master/examples/ppi.py\n",
    "#https://github.com/rusty1s/pytorch_geometric/blob/master/examples/proteins_topk_pool.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (minjae)",
   "language": "python",
   "name": "minjae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
